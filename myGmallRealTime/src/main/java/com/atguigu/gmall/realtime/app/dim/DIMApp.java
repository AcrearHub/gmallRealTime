package com.atguigu.gmall.realtime.app.dim;

import com.alibaba.fastjson.JSON;
import com.alibaba.fastjson.JSONObject;
import com.atguigu.gmall.realtime.beans.TableProcess;
import com.atguigu.gmall.realtime.fuc.DimSinkFunction;
import com.atguigu.gmall.realtime.fuc.TableProcessFunction;
import com.atguigu.gmall.realtime.utils.MyKafkaUtil;
import com.ververica.cdc.connectors.mysql.source.MySqlSource;
import com.ververica.cdc.debezium.JsonDebeziumDeserializationSchema;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.state.MapStateDescriptor;
import org.apache.flink.connector.kafka.source.KafkaSource;
import org.apache.flink.streaming.api.datastream.BroadcastStream;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.ProcessFunction;
import org.apache.flink.util.Collector;

/**
 * DIMç»´åº¦å±‚å¤„ç†
 */
public class DIMApp {
    public static void main(String[] args) throws Exception {
        //todo åˆ›å»ºæµå¼å¤„ç†
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        //è®¾ç½®å…¨å±€å¹¶è¡Œåº¦ï¼šä¸è®¾ç½®é»˜è®¤ä¸ºå…¨å¹¶è¡Œåº¦ï¼›1ä¸ºå•çº¿ç¨‹æ‰§è¡Œ
        env.setParallelism(4);
        /*
        //æ£€æŸ¥ç‚¹ç›¸å…³è®¾ç½®
        //å¦‚æœæ˜¯ç²¾ç¡®ä¸€æ¬¡ï¼Œåˆ™å¿…é¡»å¼€å¯æ£€æŸ¥ç‚¹ï¼šğŸ‘‡
        env.enableCheckpointing(5000L, CheckpointingMode.EXACTLY_ONCE);
        env.getCheckpointConfig().setCheckpointTimeout(60000L); //è¶…æ—¶æ—¶é—´
        env.getCheckpointConfig().setExternalizedCheckpointCleanup(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);  //Jobå–æ¶ˆåï¼Œä¿ç•™æ£€æŸ¥ç‚¹
        env.getCheckpointConfig().setMinPauseBetweenCheckpoints(2000L); //ä¸¤ä¸ªæ£€æŸ¥ç‚¹é—´éš”æœ€çŸ­æ—¶é—´ï¼ˆé˜²æ­¢æ£€æŸ¥ç‚¹ä¿å­˜æ—¶é—´è¿‡é•¿ï¼Œå¯¼è‡´è¿ç»­å¤‡ä»½æ£€æŸ¥ç‚¹ï¼‰
//      env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,3000L));   //é‡å¯ç­–ç•¥ï¼šå‡ºé”™æ—¶æœ€å¤šé‡å¯3æ¬¡ï¼Œæ¯æ¬¡é—´éš”3s
        env.setRestartStrategy(RestartStrategies.failureRateRestart(3, Time.days(30),Time.seconds(3))); //é‡å¯ç­–ç•¥ï¼šæ¯30å¤©æœ‰3æ¬¡æœºä¼šé‡å¯ï¼Œæ¯æ¬¡é‡å¯é—´éš”3s
        //è®¾ç½®çŠ¶æ€åç«¯
        env.setStateBackend(new HashMapStateBackend());
        env.getCheckpointConfig().setCheckpointStorage("hdfs://hadoop102:8020/checkpoint");
        //è®¾ç½®æ“ä½œhadoopçš„ç”¨æˆ·
        System.setProperty("HADOOP_USER_NAME","atguigu");
         */

        //todo ä»Kafkaä¸­è¯»å–æ•°æ®
        String topic = "topic_db";
        String groupId = "dim_app_group";   //å£°æ˜æ¶ˆè´¹ä¸»é¢˜ã€æ¶ˆè´¹è€…ç»„
        KafkaSource<String> kafkaSource = MyKafkaUtil.getKafkaSource(topic, groupId);
        DataStreamSource<String> kafkaStrDs = env.fromSource(kafkaSource, WatermarkStrategy.noWatermarks(), "kafka_source");    //å¼€å§‹æ¶ˆè´¹,å°è£…ä¸ºæµ
        //å¯¹æµä¸­æ•°æ®è¿›è¡Œç±»å‹è½¬æ¢ï¼šjson->jsonObjï¼Œå¹¶è¿›è¡ŒETLï¼Œå½¢æˆä¸»æµ
        SingleOutputStreamOperator<JSONObject> jsonObject = kafkaStrDs
                .process(new ProcessFunction<String, JSONObject>() {
                    @Override
                    public void processElement(String value, ProcessFunction<String, JSONObject>.Context ctx, Collector<JSONObject> out) {
                        try {
                            JSONObject jsonObject = JSON.parseObject(value);
                            String type = jsonObject.getString("type");
                            //å°†å¼€å§‹å’Œç»“æŸçš„typeè¿‡æ»¤æ‰ï¼ˆå†å²ï¼‰
                            if (!"bootstrap-start".equals(type) && !"bootstrap-complete".equals(type)) {
                                out.collect(jsonObject);
                            }
                        } catch (Exception e) {
                            e.printStackTrace();
                        }
                    }
                });
        //jsonObject.print("å†™å…¥æ•°æ®");

        //todo ä½¿ç”¨FlinkCDCè¯»å–é…ç½®è¡¨çš„æ•°æ®
        MySqlSource<String> mySqlSource = MySqlSource.<String>builder()
                .hostname("hadoop102")
                .port(3306)
                .databaseList("gmall_realtime_config") // set captured database
                .tableList("gmall_realtime_config.table_process") // set captured table
                .username("root")
                .password("000000")
                .deserializer(new JsonDebeziumDeserializationSchema()) // converts SourceRecord to JSON String
                .build();
        DataStreamSource<String> mySqlStrDs = env.fromSource(mySqlSource, WatermarkStrategy.noWatermarks(), "MySql Source");
        //å¯¹é…ç½®æµè¿›è¡Œå¹¿æ’­ï¼šå…ˆå»ºç«‹é…ç½®æè¿°å™¨ï¼ˆåªè¦åç§°ç›¸åŒï¼Œå°±æ˜¯åŒä¸€ä¸ªæè¿°å™¨ï¼Œé€šè¿‡keyæ¥è·å–éœ€è¦æ›´æ”¹çš„valueï¼‰
        MapStateDescriptor<String, TableProcess> mapStateDescriptor = new MapStateDescriptor<>("mapStateDescriptor", String.class, TableProcess.class);
        BroadcastStream<String> broadcast = mySqlStrDs.broadcast(mapStateDescriptor);//å¯¹å¹¿æ’­æµçš„kï¼šæ¥æºè¡¨åï¼›vï¼šè¡¨çš„æ¯è¡Œ

        //todo å°†ä¸»æµå’Œå¹¿æ’­æµè¿›è¡Œå…³è”ï¼Œå†è¿‡æ»¤å‡ºç»´åº¦æ•°æ®
        jsonObject
                .connect(broadcast)
                .process(new TableProcessFunction(mapStateDescriptor))
        //è¾“å‡ºæ•°æ®åˆ°hbaseä¸­
                .addSink(new DimSinkFunction());    //ä¸èƒ½ç”¨JDBCSinkçš„sinkæ–¹æ³•ï¼Œå®ƒåªèƒ½å¾€ä¸€å¼ è¡¨å†™

        //å¯åŠ¨ç¨‹åºæ‰§è¡Œ
        env.execute();
    }
}
